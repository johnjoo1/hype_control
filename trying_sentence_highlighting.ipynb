{
 "metadata": {
  "name": "trying_sentence_highlighting"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run mnb_live.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Get important words (pretty much the same as \"why_opinion_faster\"\n",
      "ratio = m.clf.feature_log_prob_[1]-m.clf.feature_log_prob_[0]\n",
      "opinion_words = [ ( 0, #round(count*np.exp(ratio[i]),2) ,\n",
      "                    round(count,2) , \n",
      "                    round(ratio[i],0) , \n",
      "                    m.vectorizer.get_feature_names()[i] ) for i,count in enumerate(m.vect.toarray()[0]) if count>0]\n",
      "opinion_words_sorted = sorted(opinion_words, key=lambda x:x[2])\n",
      "opinion_words_sorted[-20:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[(0, 0.05, 2.0, u'physiological'),\n",
        " (0, 0.03, 2.0, u'rep'),\n",
        " (0, 0.09, 2.0, u'sarin'),\n",
        " (0, 0.04, 2.0, u'select'),\n",
        " (0, 0.1, 2.0, u'sen'),\n",
        " (0, 0.03, 2.0, u'sept'),\n",
        " (0, 0.05, 2.0, u'steadfastly'),\n",
        " (0, 0.05, 2.0, u'throated'),\n",
        " (0, 0.05, 2.0, u'unravels'),\n",
        " (0, 0.02, 2.0, u'words'),\n",
        " (0, 0.04, 3.0, u'aug'),\n",
        " (0, 0.04, 3.0, u'contemplate'),\n",
        " (0, 0.04, 3.0, u'degrade'),\n",
        " (0, 0.04, 3.0, u'murphy'),\n",
        " (0, 0.04, 3.0, u'skeptic'),\n",
        " (0, 0.04, 3.0, u'slaughtered'),\n",
        " (0, 0.04, 3.0, u'thug'),\n",
        " (0, 0.04, 4.0, u'foxnews'),\n",
        " (0, 0.04, 4.0, u'invite'),\n",
        " (0, 0.04, 4.0, u'reflection')]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Get important words (pretty much the same as \"why_opinion_faster\"\n",
      "ratio = m.clf.feature_log_prob_[0]-m.clf.feature_log_prob_[1]\n",
      "opinion_words = [ ( 0, #round(count*np.exp(ratio[i]),2) ,\n",
      "                    round(count,2) , \n",
      "                    round(ratio[i],0) , \n",
      "                    m.vectorizer.get_feature_names()[i] ) for i,count in enumerate(m.vect.toarray()[0]) if count>0]\n",
      "opinion_words_sorted = sorted(opinion_words, key=lambda x:x[2])\n",
      "opinion_words_sorted[-20:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[(0, 0.07, 2.0, u'lawmakers'),\n",
        " (0, 0.09, 2.0, u'mcdonough'),\n",
        " (0, 0.02, 2.0, u'monday'),\n",
        " (0, 0.05, 2.0, u'nerve'),\n",
        " (0, 0.04, 2.0, u'ordering'),\n",
        " (0, 0.07, 2.0, u'said'),\n",
        " (0, 0.11, 2.0, u'saturday'),\n",
        " (0, 0.02, 2.0, u'scheduled'),\n",
        " (0, 0.03, 2.0, u'ships'),\n",
        " (0, 0.03, 2.0, u'speculation'),\n",
        " (0, 0.23, 2.0, u'sunday'),\n",
        " (0, 0.02, 2.0, u'troops'),\n",
        " (0, 0.02, 2.0, u'tuesday'),\n",
        " (0, 0.04, 3.0, u'http'),\n",
        " (0, 0.04, 3.0, u'impassioned'),\n",
        " (0, 0.02, 3.0, u'parliament'),\n",
        " (0, 0.04, 3.0, u'proceeding'),\n",
        " (0, 0.04, 3.0, u'reservations'),\n",
        " (0, 0.04, 4.0, u'revise'),\n",
        " (0, 0.04, 4.0, u'samples')]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "word_i = np.argsort(m.vect.toarray()[0])[-5:]\n",
      "top_words=[m.vectorizer.get_feature_names()[i].encode(\"utf8\",\"ignore\") for i in word_i]\n",
      "search_string = ''\n",
      "top_words.reverse()\n",
      "for word in top_words:\n",
      "    search_string+=word +' '\n",
      "search_string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "'welfare income americans consider working '"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rank_sents():\n",
      "    #pred, pred_prob = m.predict(fname=\"./sample_texts/news/fox_news.txt\")\n",
      "    pred, pred_prob = m.predict(fname=\"./sample_texts/op-ed/krugman.txt\")\n",
      "    opinion_words = m.why_opinion_faster()\n",
      "    sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "    sents =sent_tokenizer.tokenize( m.a_text)\n",
      "    choice1_logs = []\n",
      "    for sent in sents:\n",
      "        vt=m.vectorizer.transform([sent])\n",
      "        log_score = (sum(m.clf.feature_log_prob_[1]*vt.toarray()[0])+m.clf.intercept_)/sum(vt.toarray()[0])\n",
      "        choice1_logs.append([log_score, sent])\n",
      "    s_choice1_logs = sorted(choice1_logs, key = lambda x:x[0])\n",
      "    s_choice1_logs.reverse()\n",
      "    return s_choice1_logs\n",
      "r=rank_sents()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Check to make sure CountVectorizer + TfidfTransform = TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "count_vectorizer = CountVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "count_sparse = count_vectorizer.fit_transform((text for text in m.train_text))\n",
      "tdidf_transform = TfidfTransformer()\n",
      "tfidf = transformer.fit_transform(count_sparse.toarray())\n",
      "\n",
      "tfidf_vectorizer = TfidfVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "tfidf_v = tfidf_vectorizer.fit_transform((text for text in m.train_text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'transformer' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-df52c2d57d0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcount_sparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtdidf_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##  Yup, they are the same\n",
      "print np.shape(tfidf_v.toarray())\n",
      "print np.shape(tfidf.toarray())\n",
      "\n",
      "print len(tfidf_vectorizer.get_feature_names())\n",
      "\n",
      "print np.sum(tfidf_v.toarray()==tfidf.toarray())\n",
      "1358*34782\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##  Store this trained data\n",
      "stime= time.time()\n",
      "    ## actually train the data with targets\n",
      "tfidf_vectorizer = TfidfVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "tfidf_v = tfidf_vectorizer.fit_transform((text for text in m.train_text))\n",
      "X_train = tfidf_v \n",
      "y_train = m.train_target \n",
      "parameters={'alpha': 0.01}\n",
      "clf = MultinomialNB(**parameters).fit(X_train, y_train) #store log_prob and intercept\n",
      "    ##\n",
      "    \n",
      "training_counts = tfidf_v.toarray()  ## store this?  NOPE.  this has already been tfid'ed. I need counts!\n",
      "training_word_list = tfidf_vectorizer.get_feature_names()  ## store this? Nope\n",
      "\n",
      "count_vectorizer = CountVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "training_counts = count_vectorizer.fit_transform((text for text in m.train_text))  ## store this!\n",
      "assert(tfidf_vectorizer.get_feature_names() == tfidf_vectorizer.get_feature_names() ) #make sure word order is same\n",
      "training_word_list = count_vectorizer.get_feature_names()  ## store this!\n",
      "import collections\n",
      "word_count = collections.OrderedDict([(word, training_counts[:,i]) for i,word in enumerate(training_word_list)])  ##store this\n",
      "\n",
      "with open('trained_objects.pkl', 'wb') as p:\n",
      "    pickle.dump([clf.feature_log_prob_, clf.intercept_, training_word_list, training_counts], p)\n",
      "print 'prep data time: '+str(time.time()-stime)\n",
      "\n",
      "## Create new count vector for the sample text\n",
      "stime=time.time()\n",
      "sample_text = 'I love Obama Obama and cats'\n",
      "count_vectorizer_sample = CountVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "c = count_vectorizer_sample.fit_transform([sample_text])\n",
      "sample_words = count_vectorizer_sample.get_feature_names()\n",
      "test_simple_counts=c.toarray()[0] ## only first row, but there should be only 1 row.  how to assert this when shape is (3,)?\n",
      "\n",
      "test_count = np.zeros(len(word_count.keys()))\n",
      "for i, word in enumerate(sample_words):\n",
      "    idx = word_count.keys().index(word)\n",
      "    test_count[idx] = test_simple_counts[i]\n",
      "\n",
      "## Tfidf the old data with Tfidf transformer\n",
      "tfidf_transformer = TfidfTransformer()\n",
      "tfidf_training = tfidf_transformer.fit_transform(training_counts.toarray())\n",
      "#tfidf_transformer.transform((test_count)).toarray()\n",
      "\n",
      "## confirm that this procedure does the same thing as tfidf_vectorizer\n",
      "#print tfidf_transformer.transform((test_count)).toarray()[0]\n",
      "#print tfidf_vectorizer.transform([sample_text]).toarray()[0]\n",
      "#trues = sum(tfidf_vectorizer.transform([sample_text]).toarray()[0] == tfidf_transformer.transform((test_count)).toarray()[0])\n",
      "#assert(trues == len(training_word_list ))\n",
      "\n",
      "## Tfidf the new sample\n",
      "#tfidf_sample = tfidf_transformer.transform((test_count)).toarray()\n",
      "print time.time()-stime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sum(count_vectorizer.get_feature_names()==tfidf_vectorizer.get_feature_names() ) == len(count_vectorizer.get_feature_names())\n",
      "type(count_vectorizer.get_feature_names())\n",
      "['a','b','c']==['a','c','b']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stime=time.time()\n",
      "with open('trained_objects.pkl', 'r') as p:\n",
      "        [feature_log_prob, intercept, training_word_list, training_counts] = pickle.load(p)\n",
      "\n",
      "fname = \"./sample_texts/op-ed/krugman.txt\"\n",
      "f = open(fname, 'r')\n",
      "a_text=[]\n",
      "target=[]\n",
      "for line in f:\n",
      "    a_text.append(line)\n",
      "f.close()\n",
      "a_text = ' '.join(a_text)\n",
      "        \n",
      "count_vectorizer_sample = CountVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "counts_sample = count_vectorizer_sample.fit_transform([a_text]).toarray()[0]\n",
      "tokens_sample = count_vectorizer_sample.get_feature_names()\n",
      "counts_in_context = np.zeros(len(training_word_list)) #initialize zero array for all the words in document corpus\n",
      "for i, word in enumerate(tokens_sample):\n",
      "    try:\n",
      "        idx = training_word_list.index(word)\n",
      "        counts_in_context[idx] = counts_sample[i]\n",
      "    except ValueError:\n",
      "        pass\n",
      "\n",
      "## Convert counts_in_context to tfidf\n",
      "tfidf_sample = tfidf_transformer.transform((counts_in_context)).toarray()[0]\n",
      "print np.shape(tfidf_sample)\n",
      "## Calculate prediction probability\n",
      "choice0 = sum(feature_log_prob[0]*tfidf_sample)+intercept\n",
      "choice1 = sum(feature_log_prob[1]*tfidf_sample)+intercept\n",
      "print choice0, choice1\n",
      "pred_prob = [[np.exp(choice0)/(np.exp(choice0)+np.exp(choice1)), np.exp(choice1)/(np.exp(choice0)+np.exp(choice1))]]\n",
      "print pred_prob\n",
      "\n",
      "## Calculate by normal means\n",
      "sample_tfidf_vect = tfidf_vectorizer.transform([a_text])\n",
      "clf.predict(sample_tfidf_vect)\n",
      "print clf.predict_proba(sample_tfidf_vect)\n",
      "print 'total time: '+str(time.time()-stime)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.argsort(tfidf_sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.argsort(sample_tfidf)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "stime=time.time()\n",
      "tfidf_vectorizer = TfidfVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "tfidf_vectorizer.fit_transform((text for text in m.train_text))\n",
      "print time.time()-stime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "log_dict = dict(zip(m.vectorizer.get_feature_names(), zip(m.clf.feature_log_prob_[0], m.clf.feature_log_prob_[1])))\n",
      "intercept = m.clf.intercept_\n",
      "log_dict['tabled']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(encoding='latin1', stop_words = nltk.corpus.stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts= vectorizer.fit_transform(['i love cats and obama obama.']).toarray()[0]\n",
      "counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = vectorizer.get_feature_names()\n",
      "t_array = zip(counts, tokens)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for token in t_array:\n",
      "    print log_dict[token[1]], token[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m.clf.intercept_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "pred, pred_prob = m.predict(fname=\"./sample_texts/op-ed/krugman.txt\")\n",
      "print pred_prob\n",
      "choice0 = sum(m.clf.feature_log_prob_[0]*m.vect.toarray()[0])+m.clf.intercept_\n",
      "choice1 = sum(m.clf.feature_log_prob_[1]*m.vect.toarray()[0])+m.clf.intercept_\n",
      "print choice0, choice1\n",
      "pred_prob_calc = np.exp(choice1)/(np.exp(choice0)+np.exp(choice1))\n",
      "print pred_prob_calc\n",
      "m.vect.toarray()[0]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print [m.vectorizer.get_feature_names()[x] for x in np.argsort(m.clf.feature_log_prob_[1])[-50:]]\n",
      "print [m.vectorizer.get_feature_names()[x] for x in np.argsort(m.clf.feature_log_prob_[0])[-50:]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = m.clf.feature_log_prob_[0]/m.clf.feature_log_prob_[1]\n",
      "x= np.argsort(r)\n",
      "print [m.vectorizer.get_feature_names()[x] for x in np.argsort(r)[:100]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## count the words: for important words\n",
      "count_vectorizer = CountVectorizer(encoding=u'utf-8', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "training_counts = count_vectorizer.fit_transform((text for text in m.train_text))  ## store this!\n",
      "training_word_list = count_vectorizer.get_feature_names()  ## store this!\n",
      "m.train_target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mnb_live\n",
      "\n",
      "import urllib2\n",
      "import pickle\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "import nltk\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "with open('trained_objects.pkl', 'r') as p:\n",
      "        [log_dict, intercept] = pickle.load(p)\n",
      "\n",
      "fname = \"./sample_texts/op-ed/krugman.txt\"\n",
      "f = open(fname, 'r')\n",
      "a_text=[]\n",
      "target=[]\n",
      "for line in f:\n",
      "    a_text.append(line)\n",
      "f.close()\n",
      "a_text = ' '.join(a_text)\n",
      "        \n",
      "vectorizer = TfidfVectorizer(encoding='latin1', stop_words = nltk.corpus.stopwords.words('english'))\n",
      "word_counts = vectorizer.fit_transform([a_text]).toarray()[0]\n",
      "tokens = vectorizer.get_feature_names()\n",
      "t_array = dict(zip(tokens, word_counts))\n",
      "choice0 = intercept\n",
      "choice1 = intercept\n",
      "for token in t_array:\n",
      "    try:\n",
      "        choice0 += log_dict[token][0]*t_array[token]\n",
      "        choice1 += log_dict[token][1]*t_array[token]\n",
      "    except KeyError:\n",
      "        pass\n",
      "\n",
      "print choice0, choice1\n",
      "pred_prob = [[np.exp(choice0)/(np.exp(choice0)+np.exp(choice1)), np.exp(choice1)/(np.exp(choice0)+np.exp(choice1))]]\n",
      "print pred_prob\n",
      "# pred, pred_prob = self.model.predict(raw_text=self.a.cleaned_text)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "transformer = TfidfTransformer()\n",
      "counts = [[3, 0, 1],\n",
      "...           [2, 0, 0],\n",
      "...           [3, 0, 0],\n",
      "...           [4, 0, 0],\n",
      "...           [3, 2, 0],\n",
      "...           [3, 0, 2]]\n",
      "tfidf = transformer.fit_transform(counts)\n",
      "weights = transformer.idf_ "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "transformer.transform([3,1,1]).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}